{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# please check https://www.kaggle.com/ikobzev/moa-keras-nn-baseline-from-ml-newbie #updated on 14th Nov\n# deal G separately and C separately for auto encoders\n\n#https://www.kaggle.com/optimo/selfsupervisedtabnet --> please refer this 20/11","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import QuantileTransformer\n!pip install ../input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\n!pip install ../input/iterative-stratification/iterative-stratification-master/\n\n#---------------- torch imports -----------------------------\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n#---------------------------------------------------------------\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# ---------------------------------------START OF RAPIDS installation---------------------\nimport sys\n# !cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n# !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n# sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n# !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n# -----------------------------------------END of RAPIDS installation-------------------------\n# !pip install iterative-stratification\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n#from cuml.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport seaborn as sns\n\n# -----------------------------------------KERAS Imports ------------------------------------\nfrom keras import Input, metrics\nfrom keras.layers import Dense,Dropout\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import EarlyStopping\n# ------------------------------------End of Keras Imports-----------------------------------\nimport torch\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n# !pip install pytorch-tabnet\n# from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\nfrom sklearn.model_selection import train_test_split\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(88)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntrain_nonscored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_nonscored.csv\")\ntrain_scored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")\n\ntest_features = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['cp_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_vehicle = True\n# Since if cp_type!=trt_cp, all the targets are 0\n\nif remove_vehicle:\n    kept_index = train_features['cp_type']=='trt_cp'\n    train = train_features.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_scored.loc[kept_index].reset_index(drop=True)\n\ntrain_features[\"cp_type\"] = (train_features[\"cp_type\"]==\"trt_cp\") + 0\ntrain_features[\"cp_dose\"] = (train_features[\"cp_dose\"]==\"D1\") + 0\n\ntest_features[\"cp_type\"] = (test_features[\"cp_type\"]==\"trt_cp\") + 0\ntest_features[\"cp_dose\"] = (test_features[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test_features.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kept_index = train_features['cp_type']==1\ntrain_features = train_features.loc[kept_index].reset_index(drop=True)\n\nkept_index = test_features['cp_type']==1\ntest_features = test_features.loc[kept_index].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stratified kfold"},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taken from Chris Deotte folds\n\nSEED = 42\n\nNB_SPLITS = 5  #5\ndrug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\ntargets = train_targets_scored.columns[1:]\n# Taken from Chris : https://www.kaggle.com/c/lish-moa/discussion/195195\nscored = train_targets_scored.merge(drug, on='sig_id', how='left') \n# LOCATE DRUGS\nvc = scored.drug_id.value_counts()\nvc1 = vc.loc[vc<=18].index.sort_values()\nvc2 = vc.loc[vc>18].index.sort_values()\n\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, shuffle=True, \n          random_state=SEED)\ntmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, shuffle=True, \n          random_state=SEED)\ntmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n\n# ASSIGN FOLDS\nscored['fold'] = scored.drug_id.map(dct1)\nscored.loc[scored.fold.isna(),'fold'] =\\\n    scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\nscored.fold = scored.fold.astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scored.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rank Gauss"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir(\"../input/rank-gauss\")\n\nfrom gauss_rank_scaler import GaussRankScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES = [col for col in train_features.columns if col.startswith(\"g-\")]\n# CELLS = [col for col in train_features.columns if col.startswith(\"c-\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features.shape)\nprint(test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"21948+3624","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = pd.concat([train_features, test_features], ignore_index = True)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\nmask = (data_all[cols_numeric].var() >= 0.7).values\ntmp = data_all[cols_numeric].loc[:, mask]\ndata_all = pd.concat([data_all[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = GaussRankScaler()\ndata_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncompo_genes = 80\nncompo_cells = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\nCELLS = [col for col in data_all.columns if col.startswith(\"c-\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_genes = PCA(n_components = ncompo_genes,random_state = 42).fit_transform(data_all[GENES])\npca_cells = PCA(n_components = ncompo_cells,random_state = 42).fit_transform(data_all[CELLS])\n    \npca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\npca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\ndata_all = pd.concat([data_all, pca_genes, pca_cells], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering - Stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nGENES = [col for col in data_all.columns if col.startswith(\"g-\")]\nCELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n\nfor stats in tqdm.tqdm([\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]):\n    data_all[\"g_\" + stats] = getattr(data_all[GENES], stats)(axis = 1)\n    data_all[\"c_\" + stats] = getattr(data_all[CELLS], stats)(axis = 1)    \n    data_all[\"gc_\" + stats] = getattr(data_all[GENES + CELLS], stats)(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df and test_df\ndata_all_copy = data_all.copy()\nfeatures_to_drop = [\"sig_id\"]\ndata_all.drop(features_to_drop, axis = 1, inplace = True)\n\ntrain_df = data_all[: train_features.shape[0]]\ntrain_df.reset_index(drop = True, inplace = True)\n# The following line it's a bad practice in my opinion, targets on train set\n#train_df = pd.concat([train_df, targets], axis = 1)\ntest_df = data_all[train_df.shape[0]: ]\ntest_df.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Having a backup in the end\ntrain_df_copy = data_all_copy[: train_features.shape[0]]\ntrain_df_copy.reset_index(drop = True, inplace = True)\n# The following line it's a bad practice in my opinion, targets on train set\n#train_df = pd.concat([train_df, targets], axis = 1)\ntest_df_copy = data_all_copy[train_df.shape[0]: ]\ntest_df_copy.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_copy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pretrainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_df.copy()\ntest_features = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features.shape)\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_features.drop(columns=[\"sig_id\"], inplace=True)\n# test_features.drop(columns=['sig_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.values[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_tabnet.pretraining import TabNetPretrainer\n\nBS=1024\nMAX_EPOCH=101\n\nN_D = 128\nN_A = 48  #32\nN_INDEP = 1\nN_SHARED = 1\nN_STEPS = 3\n\ntabnet_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,  #0.2,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         lambda_sparse=0., optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=2e-2),\n                         mask_type=\"entmax\",\n                         scheduler_params=dict(mode=\"min\",\n                                               patience=5,\n                                               min_lr=1e-5,\n                                               factor=0.9,),\n                         scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,                         \n                         verbose=10,\n                         )\n\npretrainer = TabNetPretrainer(**tabnet_params)\n#test_features.values[:,1:]  on 25th Nov\npretrainer.fit(X_train=test_features.values[:,1:], #  np.vstack([train.values[:,1:], test.values[:,1:]])\n          eval_set=[train_features.values[:,1:]],\n          max_epochs=MAX_EPOCH,\n          patience=20, batch_size=BS, virtual_batch_size=128, #128,\n          num_workers=0, drop_last=True,\n          pretraining_ratio=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation starting from pretrained weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test = test_features.values[:,1:]\nX_test = test_features.values[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_auc_all= []\ntest_cv_preds = []\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nNB_FOLD = 5\nfor fold_nb in range(NB_FOLD):\n    train_idx = scored[scored.fold!=fold_nb].index\n    val_idx = scored[scored.fold==fold_nb].index\n\n    print(\"FOLDS : \", fold_nb)\n    if fold_nb >= NB_FOLD:\n        break\n    ## model\n#     X_train, y_train = train_features.values[train_idx, 1:], train_scored.values[train_idx, 1:].astype(float) #[:,simple_tasks].astype(float)\n#     X_val, y_val = train_features.values[val_idx, 1:], train_scored.values[val_idx, 1:].astype(float) # [:,simple_tasks].astype(float)\n    \n    X_train, y_train = train_features.values[train_idx, 1:], train_targets_scored.values[train_idx, 1:].astype(float) #[:,simple_tasks].astype(float)\n    X_val, y_val = train_features.values[val_idx, 1:], train_targets_scored.values[val_idx, 1:].astype(float) # [:,simple_tasks].astype(float)\n    MAX_EPOCH=200\n    BS=1024\n\n    tabnet_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         gamma=1.0,\n                         lambda_sparse=0., optimizer_fn=torch.optim.Adam, # \n                         optimizer_params=dict(lr=2e-2, # 2e-2\n                                               weight_decay=1e-5\n                                              ),\n                         mask_type=\"entmax\",\n                         scheduler_params=dict(max_lr=0.05,\n                                              steps_per_epoch=int(X_train.shape[0] / BS),\n                                              epochs=MAX_EPOCH,\n                                              is_batch_level=True),\n                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                         verbose=10,\n                         )\n\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=BS, virtual_batch_size=128, #128,\n              num_workers=1, drop_last=True,\n              from_unsupervised=pretrainer,            \n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n        ## save oof to compute the CV later \n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 / (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n    scores.append(score)\n    oof_preds.append(preds)\n    oof_targets.append(y_val)\n\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)    \n\n    # preds on test\n    preds_test = model.predict(X_test)\n#     preds_test = model.predict(X_test[:,1:])\n    test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Consolidating results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=oof_targets_all[:, task_id],\n                              y_score=oof_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\ntest_df_copy[all_feat] = test_preds_all.mean(axis=0)\ncolumn_names = []\ncolumn_names.append(\"sig_id\")\ncolumn_names.extend(all_feat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.merge(submission['sig_id'] ,test_df_copy[column_names], on=\"sig_id\",how=\"left\")\nsubmission_df.fillna(0,inplace=True)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"..\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('../working/submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decompose features using AutoEncoders Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining AE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled.drop(['sig_id','cp_time','cp_dose','kfold','is_train','index'],axis=1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled.drop(['sig_id','cp_time','cp_dose','kfold','is_train','index'],axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# METRICS = [metrics.RootMeanSquaredError(name='rms'),metrics.MeanAbsoluteError(name='mae')]\n\n# encoded_dim = 60   # 60 before\n\n# input_df = Input(shape=(772,))\n# encode_layer1 = Dense(encoded_dim * 4, activation='swish')(input_df)\n# encode_layer2 = Dense(encoded_dim * 3, activation='swish')(encode_layer1)\n# encode_layer3 = Dense(encoded_dim * 2, activation='swish')(encode_layer2)\n# bottleneck = Dense(encoded_dim, activation='swish')(encode_layer3)   # this is the main layer for encoding dim 60\n\n# decode_layer1 = Dense(encoded_dim * 2, activation='swish')(bottleneck)\n# decode_layer2 = Dense(encoded_dim * 3, activation='swish')(decode_layer1)\n# decode_layer3 = Dense(encoded_dim * 4, activation='swish')(decode_layer2)\n# output = Dense(772, activation='swish')(decode_layer3)\n\n# autoencoders = Model(input_df, output)  # To check the  mse of the model\n# encoders = Model(input_df, bottleneck)  # To reduce the dimensions\n\n# autoencoders.compile(optimizer='adam', loss='mean_squared_error',metrics=[METRICS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# g_features = [col for col in list(combine_df_rescaled.columns) if 'g-' in col]\n# g_features_combined = combine_df_rescaled[g_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# g_features_combined.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = g_features_combined.iloc[:22237]\n# X_valid = g_features_combined.iloc[22237:]\n\n# reduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.1, verbose=0,mode='min',\n#                               patience=3, min_lr=1E-7)\n# early_st = EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=7, verbose=0, mode='min',\n#     baseline=None, restore_best_weights=True)\n\n# autoencoders.fit(X_train, X_train,\n#                 epochs=100,\n#                 batch_size=256,\n#                 shuffle=True,\n#                 validation_data=(X_valid, X_valid),\n#                 callbacks =[reduce_lr, early_st]\n#                 )\n\n# X_reduced = encoders.predict(g_features_combined)\n\n# col = []\n# for i in range(60):\n#     col_name = f\"gae_{i}\"\n#     col.append(col_name)\n# X_reduced_df = pd.DataFrame(X_reduced, columns = col)\n\n# #non_reduced = combine_df_rescaled[['sig_id','cp_type','cp_time','cp_dose','kfold','is_train','index']]\n# combine_df_prescaled_pca = pd.concat([combine_df_rescaled, X_reduced_df],axis=1)  # autoencoders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# METRICS = [metrics.RootMeanSquaredError(name='rms'),metrics.MeanAbsoluteError(name='mae')]\n\n# encoded_dim = 15   # 15 before\n\n# input_df = Input(shape=(100,))\n# encode_layer1 = Dense(encoded_dim * 4, activation='swish')(input_df)\n# encode_layer2 = Dense(encoded_dim * 3, activation='swish')(encode_layer1)\n# encode_layer3 = Dense(encoded_dim * 2, activation='swish')(encode_layer2)\n# bottleneck = Dense(encoded_dim, activation='swish')(encode_layer3)   # this is the main layer for encoding dim 60\n\n# decode_layer1 = Dense(encoded_dim * 2, activation='swish')(bottleneck)\n# decode_layer2 = Dense(encoded_dim * 3, activation='swish')(decode_layer1)\n# decode_layer3 = Dense(encoded_dim * 4, activation='swish')(decode_layer2)\n# output = Dense(100, activation='swish')(decode_layer3)\n\n# autoencoders = Model(input_df, output)  # To check the  mse of the model\n# encoders = Model(input_df, bottleneck)  # To reduce the dimensions\n\n# autoencoders.compile(optimizer='adam', loss='mean_squared_error',metrics=[METRICS])\n\n# c_features = [col for col in list(combine_df_prescaled_pca.columns) if 'c-' in col]\n# c_features_combined = combine_df_prescaled_pca[c_features]\n\n\n# X_train = c_features_combined.iloc[:22237]\n# X_valid = c_features_combined.iloc[22237:]\n\n# reduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.1, verbose=0,mode='min',\n#                               patience=3, min_lr=1E-7)\n# early_st = EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=7, verbose=0, mode='min',\n#     baseline=None, restore_best_weights=True)\n\n# autoencoders.fit(X_train, X_train,\n#                 epochs=100,\n#                 batch_size=256,\n#                 shuffle=True,\n#                 validation_data=(X_valid, X_valid),\n#                 callbacks =[reduce_lr, early_st]\n#                 )\n\n# X_reduced = encoders.predict(c_features_combined)\n\n# col = []\n# for i in range(15):\n#     col_name = f\"cae_{i}\"\n#     col.append(col_name)\n# X_reduced_df = pd.DataFrame(X_reduced, columns = col)\n\n# #non_reduced = combine_df_rescaled[['sig_id','cp_type','cp_time','cp_dose','kfold','is_train','index']]\n# combine_df_prescaled_pca = pd.concat([combine_df_prescaled_pca, X_reduced_df],axis=1)  # autoencoders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decompose features using PCA as there are lots of features\nn_components we will try to setup using variances 0.95 or 0.99"},{"metadata":{},"cell_type":"markdown","source":"### PCA for g- features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# g_features = [col for col in list(combine_df_rescaled.columns) if 'g-' in col]\n# g_features_combined = combine_df_rescaled[g_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## using 30 components as per discussion forum\n# pca = PCA(n_components = 30)\n# pca.fit(g_features_combined.values)\n# reduced_g_features = pca.fit_transform(g_features_combined.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pca = PCA(n_components = 0.99)\n# pca.fit(g_features_combined.values)\n# reduced_g_features = pca.fit_transform(g_features_combined.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(g_features_combined.shape)\n# print(reduced_g_features.shape)\n# ## looks like more columns again","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pca = PCA(n_components = 0.95)\n# pca.fit(g_features_combined.values)\n# reduced_g_features = pca.fit_transform(g_features_combined.values)\n# print(g_features_combined.shape)\n# print(reduced_g_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will go with 0.95 variance as this has significantly reduced 270 columns approximately"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # reduced_g_features to columns in pandas dataframe\n# g_columns_components = []\n# for i in range(reduced_g_features.shape[1]):\n#     g_columns_components.append(\"g-pca-{}\".format(i))\n# g_component_df = pd.DataFrame(reduced_g_features,columns=g_columns_components)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca = pd.concat([combine_df_rescaled, g_component_df],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA for c- features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# c_features = [col for col in list(combine_df_rescaled.columns) if 'c-' in col]\n# c_features_combined = combine_df_rescaled[c_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # using 4 components as per discussion forum\n# pca = PCA(n_components = 4)\n# pca.fit(c_features_combined.values)\n# reduced_c_features = pca.fit_transform(c_features_combined.values)\n# print(c_features_combined.shape)\n# print(reduced_c_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pca = PCA(n_components = 0.99)\n# pca.fit(c_features_combined.values)\n# reduced_c_features = pca.fit_transform(c_features_combined.values)\n# print(c_features_combined.shape)\n# print(reduced_c_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pca = PCA(n_components = 0.95)\n# pca.fit(c_features_combined.values)\n# reduced_c_features = pca.fit_transform(c_features_combined.values)\n# print(c_features_combined.shape)\n# print(reduced_c_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again 95% variance is chosen as it has significantly decomposed many columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # reduced_c_features to columns in pandas dataframe\n# c_columns_components = []\n# for i in range(reduced_c_features.shape[1]):\n#     c_columns_components.append(\"c-pca-{}\".format(i))\n# c_component_df = pd.DataFrame(reduced_c_features,columns=c_columns_components)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca = pd.concat([combine_df_prescaled_pca, c_component_df],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining all together into one dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled_pca = pd.concat([combine_df_rescaled[['sig_id','cp_type','cp_time','cp_dose','kfold','is_train','index']],g_component_df],axis=1)\n# combine_df_rescaled_pca = pd.concat([combine_df_rescaled_pca,c_component_df],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_rescaled_pca.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca_copy = combine_df_prescaled_pca.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca = combine_df_prescaled_pca_copy.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# var_t = VarianceThreshold(0.5)\n# feature_selected_g_c = var_t.fit_transform(combine_df_prescaled_pca.drop(['sig_id','cp_time','cp_dose','kfold','is_train','index'],axis=1))\n# cols = []\n# print(feature_selected_g_c.shape)\n# for i in range(feature_selected_g_c.shape[1]):\n#     cols.append(\"g_c_{}\".format(i))\n# feature_selected_var_thresh = pd.DataFrame(feature_selected_g_c,columns=cols)\n# combine_df_prescaled_pca = pd.concat([combine_df_prescaled_pca[['sig_id','cp_time','cp_dose','kfold','is_train','index']], feature_selected_var_thresh],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We will predict nonscored features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.chdir(\"../../input/tabnet-v1/tabnet-develop\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pytorch_tabnet.metrics import Metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pytorch_tabnet.tab_model import TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine_df_prescaled_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = combine_df_prescaled_pca[combine_df_prescaled_pca['is_train'] == 1]\n# test = combine_df_prescaled_pca[combine_df_prescaled_pca['is_train'] == 0]\n\n# ### Defining the categorical columns and dims\n\n# categorical_columns = ['cp_time','cp_dose']\n# categorical_dims = {'cp_time':3,'cp_dose':2}\n\n# unused_feats = ['sig_id','index','kfold','is_train']\n\n# features = [ col for col in train.columns if col not in unused_feats] \n\n# cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n\n# cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat_dims","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Params"},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BS=256\n# MAX_EPOCH=101\n\n# N_D = 128\n# N_A = 32\n# N_INDEP = 1\n# N_SHARED = 1\n# N_STEPS = 3\n\n# tabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n#                      lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n#                      optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n#                      mask_type='entmax',\n#                      scheduler_params=dict(mode=\"min\",\n#                                            patience=5,\n#                                            min_lr=1e-5,\n#                                            factor=0.9,),\n#                    #  scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n#                      verbose=10,\n#                      )\n\n# pretrainer = TabNetPretrainer(**tabnet_params)\n\n# pretrainer.fit(X_train=test.drop(['sig_id','kfold','is_train','index'], axis=1).values, #  np.vstack([train.values[:,1:], test.values[:,1:]])\n#           eval_set=[train.drop(['sig_id','kfold','is_train','index'], axis=1).values],\n#           max_epochs=MAX_EPOCH,\n#           patience=20, batch_size=BS, virtual_batch_size=32, #128,\n#           num_workers=0, drop_last=True,\n#           pretraining_ratio=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = TabNetRegressor(**tabnet_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = TabNetRegressor(cat_dims=cat_dims, cat_emb_dim=[2,3,2], cat_idxs=cat_idxs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.1, verbose=0,mode='min',\n#                               patience=3, min_lr=1E-7)\n# early_st = EarlyStopping(monitor='val_logloss', min_delta=1E-4, patience=7, verbose=0, mode='min',\n#     baseline=None, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import log_loss\n# from pytorch_tabnet.metrics import Metric\n# from sklearn.metrics import roc_auc_score, log_loss\n\n# class LogitsLogLoss(Metric):\n#     \"\"\"\n#     LogLoss with sigmoid applied\n#     \"\"\"\n\n#     def __init__(self):\n#         self._name = \"logits_ll\"\n#         self._maximize = False\n\n#     def __call__(self, y_true, y_pred):\n#         \"\"\"\n#         Compute LogLoss of predictions.\n\n#         Parameters\n#         ----------\n#         y_true: np.ndarray\n#             Target matrix or vector\n#         y_score: np.ndarray\n#             Score matrix or vector\n\n#         Returns\n#         -------\n#             float\n#             LogLoss of predictions vs targets.\n#         \"\"\"\n#         logits = 1 / (1 + np.exp(-y_pred))\n#         aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n#         return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def create_df(preds):\n#     col_names = []\n#     for i in range(206):\n#         col_names.append(str(i))\n#     df = pd.DataFrame(preds.reshape(-1,206), columns=col_names)\n#     return df\n# def perform_fitting_fold_tabnet(train, test, train_scored, fold, NB_TASKS = 206):\n#     train_v1 = train[train['kfold'] == fold]\n#     train_scored_v1 = train_scored[train_scored['sig_id'].isin(train_v1['sig_id'].unique())]\n    \n#     valid = train[train['kfold'] != fold]\n#     valid_scored = train_scored[train_scored['sig_id'].isin(valid['sig_id'].unique())]\n    \n#     print(valid)\n#     X_train = train_v1.drop(['sig_id','index','kfold','is_train'],axis=1)\n#     y_train = train_scored_v1.drop(\"sig_id\",axis=1)\n    \n#     X_valid = valid.drop(['sig_id','index','kfold','is_train'],axis=1)\n#     y_valid = valid_scored.drop(\"sig_id\",axis=1)\n    \n    \n#     X_test = test.drop(['sig_id','index','kfold','is_train'],axis=1)\n    \n#     clf.fit(X_train.values,y_train.values,\n#             eval_set=[(X_valid.values, y_valid.values)],\n#             eval_name = [\"val\"],\n#             eval_metric = [\"logits_ll\"],\n#             max_epochs=100, patience=20, batch_size=1024, virtual_batch_size=128,\n#             loss_fn=torch.nn.functional.binary_cross_entropy_with_logits,\n#             from_unsupervised=pretrainer)\n    \n#     preds = clf.predict(X_test.values)\n    \n#     # perform sigmoid\n#     preds_sig =  1 / (1 + np.exp(-preds))\n    \n#     df = create_df(preds_sig)\n#     return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p_min = 0.0005\n# p_max = 0.9995\n# import tensorflow as tf\n# from keras import backend \n# def logloss(y_true, y_pred):\n#     y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n#     return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras import losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def create_model(input_shape):\n#     input_df = Input(shape=(input_shape,))\n#     layer1 = Dense(512, activation=\"elu\")(input_df)\n#     layer2 = Dense(1024, activation=\"elu\")(layer1)\n#     layer3 = Dense(512, activation=\"elu\")(layer2)\n#     layer4 = Dense(2048, activation=\"elu\")(layer3)\n#     output = Dense(206, activation=\"sigmoid\")(layer4)\n\n#     model2 = Model(input_df, output) \n#     model2.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=0.0015),metrics=logloss)\n#     return model2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def perform_fitting_fold_model2(train, test, train_scored, fold, NB_TASKS = 206):\n#     train_v1 = train[train['kfold'] == fold]\n#     train_scored_v1 = train_scored[train_scored['sig_id'].isin(train_v1['sig_id'].unique())]\n    \n#     valid = train[train['kfold'] != fold]\n#     valid_scored = train_scored[train_scored['sig_id'].isin(valid['sig_id'].unique())]\n\n#     X_train = train_v1.drop(['sig_id','index','kfold','is_train'],axis=1)\n#     y_train = train_scored_v1.drop(\"sig_id\",axis=1)\n    \n#     X_valid = valid.drop(['sig_id','index','kfold','is_train'],axis=1)\n#     y_valid = valid_scored.drop(\"sig_id\",axis=1)\n    \n#     X_test = test.drop(['sig_id','index','kfold','is_train'],axis=1)\n#     input_shape = X_train.shape[1]\n#     model2 = create_model(input_shape)\n#     model2.fit(X_train.values,y_train.values,epochs=200,batch_size=256,\n#                validation_data = (X_valid.values,y_valid.values),\n#                callbacks =[reduce_lr, early_st]\n#               )\n    \n#     preds = model2.predict(X_test.values)\n    \n#     df = create_df(preds)\n#     return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_seeds = 6\n# np.random.seed(1)\n# seeds = np.random.randint(0,100,size=n_seeds)\n\n# preds_df_list = []\n# for i,seed in enumerate(seeds):\n#     for j in range(5):\n#         if(seed%2 == 0):\n# #             preds = perform_fitting_fold_model2(train,test,train_scored,j)\n# #             preds_df_list.append(preds)\n#             preds = perform_fitting_fold_tabnet(train,test,train_scored,j)\n#             preds_df_list.append(preds)\n#         else:\n#             preds = perform_fitting_fold_tabnet(train,test,train_scored,j)\n#             preds_df_list.append(preds)\n# #             preds = perform_fitting_fold_model2(train,test,train_scored,j)\n# #             preds_df_list.append(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(preds_df_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_list = []\n# df= pd.DataFrame()\n# for i in range(0, len(preds_df_list),5):\n#     for j in range(preds_df_list[0].shape[1]):\n#         cols_name = str(j)\n#         df[cols_name] = (preds_df_list[i].iloc[:,j] + preds_df_list[i+1].iloc[:,j] + preds_df_list[i+2].iloc[:,j] + preds_df_list[i+3].iloc[:,j] + preds_df_list[i+4].iloc[:,j]) / 5\n#     df_list.append(df)\n#     df = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ############### For seeds###############\n# df_sub = pd.DataFrame()\n# for j in range(df_list[0].shape[1]):\n#     cols_name = str(j)\n#     df_sub[cols_name] = (df_list[0].iloc[:,j] + df_list[1].iloc[:,j] + df_list[2].iloc[:,j] + df_list[3].iloc[:,j] + df_list[4].iloc[:,j] + df_list[5].iloc[:,j]) / 6\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_sub.columns = submission.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_sub = pd.concat([test['sig_id'], df_sub],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mysub = pd.merge(submission['sig_id'], df_sub, on=\"sig_id\",how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mysub.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mysub = mysub[submission.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mysub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.chdir(\"/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# mysub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}